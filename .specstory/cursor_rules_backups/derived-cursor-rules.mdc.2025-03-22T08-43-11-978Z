
## PROJECT OVERVIEW

This project aims to create a lesson management system integrated with AI-powered transcription and summarization features.  The system will allow users to upload audio recordings of lessons, generate summaries and tags, and manage their lesson schedules.  Version updates will be noted in the relevant sections.  OpenAI Whisper API and Google Gemini API are used for transcription and summarization. Google Gemini API was added to the tech stack. Node.js 18 runtime will be deprecated on 2025-04-30 and decommissioned on 2025-10-31.  Consider upgrading to Node.js 20 to avoid disruption.  The maximum audio duration for transcription is 90 minutes. For files larger than 25MB, implement a process to split the audio into smaller chunks before sending to the Whisper API. For files larger than 30MB, a warning will be displayed if not on Wi-Fi. The maximum file size for audio uploads is 100MB and the maximum audio duration is 90 minutes. A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network.  All functions must be deployed to the `asia-northeast1` region. Implement a mechanism to handle large files gracefully, providing warnings to the users when necessary.  Implement robust error handling and logging for all API calls. Implement retry mechanisms for failed API calls with exponential backoff. Lesson data includes `instrument`, `summary`, `tags`, `summaryRequired`, `summaryInProgress`, `transcriptionCompleteTime`, `transcriptionId`, `processingId`, and `lockAcquiredAt` fields. `instrument` is populated from user profile. The `generateSummaryFromTranscriptionV2` function will only generate a summary if the `summaryRequired` field is `true` and the lesson's status is `transcribed`.  A new `transcriptionId` field has been added to the Lesson data structure to prevent duplicate processing. A new `transcriptionCompleteTime` field has been added to the Lesson data structure.  A new `lockAcquiredAt` field has been added to the Lesson data structure. Dify input field settings for transcription (callDifyAPI function) are no longer needed since Dify has been removed. Dify input field settings for summary and tag generation (createLessonSummary function) are no longer needed since Dify has been removed. To check OpenAI API errors, follow these steps: 1. Set the OPENAI_API_KEY environment variable in your Firebase Functions environment. 2. Redeploy your functions to apply the new settings. 3. Check the Firebase Functions logs to monitor function executions and identify errors.  For detailed API request/response logs, check the browser's DevTools console.  The Whisper API has a file upload limit; files exceeding this limit will result in a "413 Request Entity Too Large" error. User only receives summary and tags; transcription is not provided. A mechanism to handle large files gracefully, providing warnings to users when necessary, has been implemented. Robust error handling and logging for all API calls have been implemented. Retry mechanisms for failed API calls with exponential backoff have been implemented. The maximum file size for audio uploads is 100MB, and the maximum audio duration is 90 minutes. A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. All functions must be deployed to the `asia-northeast1` region. The `sys.app_id` parameter is required for Dify API requests. This parameter is no longer needed since Dify has been removed. All functions should be deployed to the `asia-northeast1` region.  All functions should be deployed to the asia-northeast1 region.  The maximum file size for audio uploads is 100MB, and the maximum audio duration is 90 minutes. A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. All functions must be deployed to the `asia-northeast1` region.  A new `transcriptionId` field has been added to the Lesson data structure to prevent duplicate processing.  A new `processAudioCustomFlow` function has been implemented to handle the custom flow: Whisper (with splitting) -> OpenAI (model unspecified) -> Gemini 1.5 Flash.  A new `processAudioV2` function has been implemented to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  A new `generateSummaryWithKnowledgeBase` function has been implemented to generate summaries using a knowledge base.  The knowledge base is now populated by an admin user via the `uploadKnowledge` function.  The `uploadKnowledge` function now includes an admin key for authentication. A new `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, and `getInstrumentTemplates` functions have been implemented for admin users to manage knowledge base entries.  The `getSummaryPromptByInstrument` function in `dify.ts` has been updated to use if statements instead of a switch statement to improve the accuracy of instrument matching.  The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions in `genkit.ts` have also been updated to use if statements for improved instrument matching. The `dify.ts` file and related functions have been removed from the project. The system now uses only the `genkit.ts` module for summary and tag generation. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary and tag generation. A new `generateSummaryWithKnowledge` function has been implemented to generate summaries using a knowledge base. The `generateSummaryWithKnowledgeBase` function now uses a knowledge base to generate summaries and tags.  A new `processAudioFileV2` function has been implemented to handle the audio file processing. A new `generateSummaryFromTranscriptionV2` function has been implemented to handle summary and tag generation.  The `processAudioV2` function now uses the `processAudioCustomFlow` function for the custom flow. The `processAudioCustomFlow` function now uses a knowledge base for summary generation. A new `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, and `getInstrumentTemplates` functions have been implemented for admin users to manage knowledge base entries.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three comma-separated words.  The current workflow is: Audio Upload -> `processAudioFileV2` (Whisper transcription with splitting) -> `generateSummaryFromTranscriptionV2` (Summary and Tag generation using Gemini). The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into 10-minute chunks with overlap for transcription to improve context and accuracy.  The summarization API has been changed to OpenAI's o3-mini model.  Long audio files are split into approximately 10-minute chunks with overlap for improved context and accuracy during transcription. The summarization API is now OpenAI's o3-mini model.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  A new admin section has been added to handle knowledge base management.  Admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) have been implemented to manage the knowledge base. The `generateSummaryFromTranscriptionV2` function now uses the `generateSummaryWithKnowledgeBase` function for summary generation. Long audio files are split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  Tags are now generated as three individual words.  Long audio files are now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  Tags are generated as three individual words; if fewer than three are generated, the instrument name will be used to fill any gaps.  The summarization API is now OpenAI's o3-mini model. Long audio files are split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper.  Tags are generated as three individual words.  Long audio files are now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  Whisper API is used for transcription. OpenAI's o3-mini model is used for summarization. Tags are generated as three individual words. If fewer than three words are generated, the instrument name will be used to fill any gaps.  The custom flow now uses OpenAI's o3-mini model for summarization.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with a 20-second overlap for improved context and accuracy during transcription using the Whisper API. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.


## CODE STYLE

Maintain consistent code style throughout the project.  Use TypeScript for type safety and maintainability. Adhere to standard TypeScript naming conventions.

## FOLDER ORGANIZATION

The project follows a feature-based folder structure.  All components, services, and hooks related to a specific feature are organized within their respective folders under `app/features`.  Assets are stored in `app/assets`.  A new admin section has been added to handle the knowledge base management.  A new `admin` folder has been created to house the admin components related to knowledge base management.  The `admin` folder now contains `knowledge-management.tsx` and `knowledge-edit.tsx` components. A new admin section has been added to handle knowledge base management.  A new `admin` folder has been created under `app` to house admin components.  The `app/admin` folder now contains `knowledge-management.tsx` and `knowledge-edit.tsx` components for managing the knowledge base.

## TECH STACK

- React Native
- TypeScript
- Firebase (Firestore, Storage, Functions)
- OpenAI Whisper API
- OpenAI o3-mini API
- Google Gemini API
- fluent-ffmpeg
- @ffmpeg-installer/ffmpeg
- fs-extra
- form-data
- @types/fluent-ffmpeg
- @types/fs-extra
- @types/form-data
- @react-native-community/netinfo
- @google/generative-ai
- openai
- date-fns
- @react-native-picker/picker


## PROJECT-SPECIFIC STANDARDS

- All audio files must be in MP3, WAV, or M4A format.
- Maximum audio file size: 100MB
- Maximum audio duration: 90 minutes
- All functions must be deployed to the `asia-northeast1` region.
- A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network.
- Lesson data includes `instrument`, `summary`, `tags`, `summaryRequired`, `summaryInProgress`, `transcriptionCompleteTime`, `transcriptionId`, `processingId`, and `lockAcquiredAt` fields.  `instrument` is populated from user profile.
- The `generateSummaryFromTranscriptionV2` function will only generate a summary if the `summaryRequired` field is `true` and the lesson's status is `transcribed`.
- The `processAudioCustomFlow` function uses Whisper for transcription (with audio splitting), OpenAI for summarization, and Gemini 1.5 Flash for tag generation and utilizes a knowledge base for improved summarization accuracy.
- The `processAudioV2` function dynamically chooses between the custom flow and the standard flow based on user settings.
- The `generateSummaryWithKnowledgeBase` function utilizes a knowledge base for improved summarization accuracy. The knowledge base is managed by admin users.
- The summary prompt is now: "[文字起こし]は[楽器]のレッスンの文字起こしデータです。この内容を、セクションごとに整理し、指摘内容・課題・練習アドバイスを簡潔にまとめてください。専門用語や基礎知識については[ナレッジ]を参考にして回答してください。【要件】- レッスンの内容をセクション単位（例：基礎練習、エチュード、曲名ごと）で分けて要約してください。- 各セクションで「指摘内容」「今後の課題」「練習アドバイス」の3つを明確に分類してください。- 雑談や無関係な話題は省き、重要な部分に焦点を当ててください。- 絵文字や記号などを使いユーザーがわかりやすく、見やすく \"復習\" できるように出力してください【フォーマット（例）】■ セクション1：基礎練習（ロングトーン・スケール）1. 指摘内容：- ロングトーンで音の立ち上がりが不安定。- スケール練習で音程が上下でぶれる傾向あり。2. 今後の課題：- ロングトーンで音の安定感を意識する。- スケール練習を一定のテンポで確実に行うこと。3. 練習アドバイス：- メトロノームを使って、ロングトーンを1音ずつ丁寧に。- スケールを2オクターブ分、ゆっくり繰り返す練習。---■ セクション2：エチュード（例：○○教本 第3番）1. 指摘内容：- ○○のフレーズで指がもつれる。- リズムが走りやすくなる箇所あり。2. 今後の課題：- フィンガリングの精度向上。- リズムの安定を意識。3. 練習アドバイス：- 片手ずつゆっくりテンポで練習。- 譜面を分割して重点的に練習。---■ セクション3：曲（曲名：クラリネット協奏曲 第1楽章）1. 指摘内容：- 出だしの音の入りが遅れる。- ブレスのタイミングが不自然。2. 今後の課題：- フレーズ頭の入りのタイミング調整。- 呼吸のポイントを譜面に記す。3. 練習アドバイス：- フレーズ冒頭だけを繰り返し練習。- 呼吸の位置でブレス練習を取り入れる。---"
- Tags are generated as three individual words.  If fewer than three words are generated, the instrument name will be used to fill any gaps.
- Long audio files are split into 10-minute chunks with overlap for transcription.
- The summarization API is now OpenAI's o3-mini model.
- The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Long audio files are split into approximately 10-minute chunks to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  The summary prompt is now: "[文字起こし]は[楽器]のレッスンの文字起こしデータです。この内容を、セクションごとに整理し、指摘内容・課題・練習アドバイスを簡潔にまとめてください。専門用語や基礎知識については[ナレッジ]を参考にして回答してください。【要件】- レッスンの内容をセクション単位（例：基礎練習、エチュード、曲名ごと）で分けて要約してください。- 各セクションで「指摘内容」「今後の課題」「練習アドバイス」の3つを明確に分類してください。- 雑談や無関係な話題は省き、重要な部分に焦点を当ててください。- 絵文字や記号などを使いユーザーがわかりやすく、見やすく \"復習\" できるように出力してください【フォーマット（例）】■ セクション1：基礎練習（ロングトーン・スケール）1. 指摘内容：- ロングトーンで音の立ち上がりが不安定。- スケール練習で音程が上下でぶれる傾向あり。2. 今後の課題：- ロングトーンで音の安定感を意識する。- スケール練習を一定のテンポで確実に行うこと。3. 練習アドバイス：- メトロノームを使って、ロングトーンを1音ずつ丁寧に。- スケールを2オクターブ分、ゆっくり繰り返す練習。---■ セクション2：エチュード（例：○○教本 第3番）1. 指摘内容：- ○○のフレーズで指がもつれる。- リズムが走りやすくなる箇所あり。2. 今後の課題：- フィンガリングの精度向上。- リズムの安定を意識。3. 練習アドバイス：- 片手ずつゆっくりテンポで練習。- 譜面を分割して重点的に練習。---■ セクション3：曲（曲名：クラリネット協奏曲 第1楽章）1. 指摘内容：- 出だしの音の入りが遅れる。- ブレスのタイミングが不自然。2. 今後の課題：- フレーズ頭の入りのタイミング調整。- 呼吸のポイントを譜面に記す。3. 練習アドバイス：- フレーズ冒頭だけを繰り返し練習。- 呼吸の位置でブレス練習を取り入れる。---"  Tags are generated as three individual words.
- Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.
- The audio is now split into approximately 10-minute chunks with a 20-second overlap for improved context and accuracy during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name will be used to fill any gaps.
- Long audio files are split into 10-minute chunks with a 20-second overlap for improved context and accuracy during transcription using the Whisper API. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  The audio is now split into approximately 10-minute chunks with a 20-second overlap for transcription.  Whisper API is used for transcription. OpenAI's o3-mini model is used for summarization. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.
- Long audio files are split into 10-minute chunks with a 20-second overlap for improved context and accuracy.  The transcription API is now OpenAI Whisper API. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for improved context and accuracy during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.
- Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi. A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network.
- The audio is now split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Added admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.
- Added new function `processAudioFileV2` to handle audio file processing.
- Added new function `generateSummaryFromTranscriptionV2` for summary and tag generation.
-  `processAudioV2` now uses `processAudioCustomFlow` for the custom flow.
- `processAudioCustomFlow` now uses a knowledge base for summary generation.
- Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.
- The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching.
- The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching.
- The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled.
- Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.
- Long audio files are split into 10-minute chunks with overlap for transcription.
- The summarization API has been changed to OpenAI's o3-mini model.
- The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are now split into approximately 10-minute chunks to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  The summary prompt is now: "[文字起こし]は[楽器]のレッスンの文字起こしデータです。この内容を、セクションごとに整理し、指摘内容・課題・練習アドバイスを簡潔にまとめてください。専門用語や基礎知識については[ナレッジ]を参考にして回答してください。【要件】- レッスンの内容をセクション単位（例：基礎練習、エチュード、曲名ごと）で分けて要約してください。- 各セクションで「指摘内容」「今後の課題」「練習アドバイス」の3つを明確に分類してください。- 雑談や無関係な話題は省き、重要な部分に焦点を当ててください。- 絵文字や記号などを使いユーザーがわかりやすく、見やすく \"復習\" できるように出力してください【フォーマット（例）】■ セクション1：基礎練習（ロングトーン・スケール）1. 指摘内容：- ロングトーンで音の立ち上がりが不安定。- スケール練習で音程が上下でぶれる傾向あり。2. 今後の課題：- ロングトーンで音の安定感を意識する。- スケール練習を一定のテンポで確実に行うこと。3. 練習アドバイス：- メトロノームを使って、ロングトーンを1音ずつ丁寧に。- スケールを2オクターブ分、ゆっくり繰り返す練習。---■ セクション2：エチュード（例：○○教本 第3番）1. 指摘内容：- ○○のフレーズで指がもつれる。- リズムが走りやすくなる箇所あり。2. 今後の課題：- フィンガリングの精度向上。- リズムの安定を意識。3. 練習アドバイス：- 片手ずつゆっくりテンポで練習。- 譜面を分割して重点的に練習。---■ セクション3：曲（曲名：クラリネット協奏曲 第1楽章）1. 指摘内容：- 出だしの音の入りが遅れる。- ブレスのタイミングが不自然。2. 今後の課題：- フレーズ頭の入りのタイミング調整。- 呼吸のポイントを譜面に記す。3. 練習アドバイス：- フレーズ冒頭だけを繰り返し練習。- 呼吸の位置でブレス練習を取り入れる。---"  Tags are generated as three individual words.
- Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.


## WORKFLOW & RELEASE RULES

- Follow the standard Gitflow branching model.
- Create pull requests for all code changes.
- Code reviews are mandatory before merging any pull request.
- All deployments should be done through Firebase CLI.  All functions should be deployed to the `asia-northeast1` region.

## REFERENCE EXAMPLES

- Examples of using the OpenAI Whisper API and Google Gemini API should be documented in the project's wiki.

## PROJECT DOCUMENTATION & CONTEXT SYSTEM

- Maintain up-to-date documentation using Markdown format.
- Version updates will be noted in the relevant sections.
- Use a consistent naming convention for all files and folders.  All functions should be deployed to the `asia-northeast1` region.

## DEBUGGING

- Use Firebase console for debugging Cloud Functions.
- Use browser's DevTools console for debugging frontend API calls.
- Implement robust logging for all API calls.

## FINAL DOs AND DON'Ts

- **DO** use TypeScript for all new code.
- **DO** write unit tests for all new features.
- **DO** use Firebase Functions for backend logic.
- **DO** document all API calls and their parameters.
- **DO** handle errors gracefully and provide informative error messages to users.
- **DON'T** commit any unnecessary files to the repository.
- **DON'T** hardcode API keys or sensitive information in the code.
- **DON'T** use deprecated libraries or APIs.
- **DON'T** introduce breaking changes without proper communication and testing.
- The maximum file size for audio uploads is 100MB, and the maximum audio duration is 90 minutes. A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network.  All functions must be deployed to the `asia-northeast1` region.
- Added admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.
- Added new function `processAudioFileV2` to handle audio file processing.
- Added new function `generateSummaryFromTranscriptionV2` for summary and tag generation.
-  `processAudioV2` now uses `processAudioCustomFlow` for the custom flow.
- `processAudioCustomFlow` now uses a knowledge base for summary generation.
- Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.
- The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching.
- The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching.
- The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled.
- Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.
- Long audio files are split into 10-minute chunks with overlap for transcription.
- The summarization API has been changed to OpenAI's o3-mini model.
- The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are now split into approximately 10-minute chunks to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  The summary prompt is now: "[文字起こし]は[楽器]のレッスンの文字起こしデータです。この内容を、セクションごとに整理し、指摘内容・課題・練習アドバイスを簡潔にまとめてください。専門用語や基礎知識については[ナレッジ]を参考にして回答してください。【要件】- レッスンの内容をセクション単位（例：基礎練習、エチュード、曲名ごと）で分けて要約してください。- 各セクションで「指摘内容」「今後の課題」「練習アドバイス」の3つを明確に分類してください。- 雑談や無関係な話題は省き、重要な部分に焦点を当ててください。- 絵文字や記号などを使いユーザーがわかりやすく、見やすく \"復習\" できるように出力してください【フォーマット（例）】■ セクション1：基礎練習（ロングトーン・スケール）1. 指摘内容：- ロングトーンで音の立ち上がりが不安定。- スケール練習で音程が上下でぶれる傾向あり。2. 今後の課題：- ロングトーンで音の安定感を意識する。- スケール練習を一定のテンポで確実に行うこと。3. 練習アドバイス：- メトロノームを使って、ロングトーンを1音ずつ丁寧に。- スケールを2オクターブ分、ゆっくり繰り返す練習。---■ セクション2：エチュード（例：○○教本 第3番）1. 指摘内容：- ○○のフレーズで指がもつれる。- リズムが走りやすくなる箇所あり。2. 今後の課題：- フィンガリングの精度向上。- リズムの安定を意識。3. 練習アドバイス：- 片手ずつゆっくりテンポで練習。- 譜面を分割して重点的に練習。---■ セクション3：曲（曲名：クラリネット協奏曲 第1楽章）1. 指摘内容：- 出だしの音の入りが遅れる。- ブレスのタイミングが不自然。2. 今後の課題：- フレーズ頭の入りのタイミング調整。- 呼吸のポイントを譜面に記す。3. 練習アドバイス：- フレーズ冒頭だけを繰り返し練習。- 呼吸の位置でブレス練習を取り入れる。---"  Tags are generated as three individual words.
- Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.
- Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.
- The audio is now split into approximately 10-minute chunks with a 20-second overlap for improved context and accuracy during transcription using Whisper. The summarization