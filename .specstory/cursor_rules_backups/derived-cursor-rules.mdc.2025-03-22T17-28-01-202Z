
## PROJECT OVERVIEW

This project aims to create a lesson management system integrated with AI-powered transcription and summarization features.  The system will allow users to upload audio recordings of lessons, generate summaries and tags, and manage their lesson schedules.  Version updates will be noted in the relevant sections.  OpenAI Whisper API and Google Gemini API are used for transcription and summarization. Google Gemini API was added to the tech stack. Node.js 18 runtime will be deprecated on 2025-04-30 and decommissioned on 2025-10-31.  Consider upgrading to Node.js 20 to avoid disruption.  The maximum audio duration for transcription is 90 minutes. For files larger than 25MB, implement a process to split the audio into smaller chunks before sending to the Whisper API. For files larger than 30MB, a warning will be displayed if not on Wi-Fi. The maximum file size for audio uploads is 100MB and the maximum audio duration is 90 minutes. A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network.  All functions must be deployed to the `asia-northeast1` region. Implement a mechanism to handle large files gracefully, providing warnings to the users when necessary.  Implement robust error handling and logging for all API calls. Implement retry mechanisms for failed API calls with exponential backoff. Lesson data includes `instrument`, `summary`, `tags`, `summaryRequired`, `summaryInProgress`, `transcriptionCompleteTime`, `transcriptionId`, `processingId`, and `lockAcquiredAt` fields. `instrument` is populated from user profile. The `generateSummaryFromTranscriptionV2` function will only generate a summary if the `summaryRequired` field is `true` and the lesson's status is `transcribed`.  A new `transcriptionId` field has been added to the Lesson data structure to prevent duplicate processing. A new `transcriptionCompleteTime` field has been added to the Lesson data structure.  A new `lockAcquiredAt` field has been added to the Lesson data structure.  Dify input field settings for transcription (callDifyAPI function) are no longer needed since Dify has been removed. Dify input field settings for summary and tag generation (createLessonSummary function) are no longer needed since Dify has been removed. To check OpenAI API errors, follow these steps: 1. Set the OPENAI_API_KEY environment variable in your Firebase Functions environment. 2. Redeploy your functions to apply the new settings. 3. Check the Firebase Functions logs to monitor function executions and identify errors.  For detailed API request/response logs, check the browser's DevTools console.  The Whisper API has a file upload limit; files exceeding this limit will result in a "413 Request Entity Too Large" error. User only receives summary and tags; transcription is not provided. A mechanism to handle large files gracefully, providing warnings to users when necessary, has been implemented. Robust error handling and logging for all API calls have been implemented. Retry mechanisms for failed API calls with exponential backoff have been implemented. The maximum file size for audio uploads is 100MB, and the maximum audio duration is 90 minutes. A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. All functions must be deployed to the `asia-northeast1` region.  A new `transcriptionId` field has been added to the Lesson data structure to prevent duplicate processing.  A new `processAudioCustomFlow` function has been implemented to handle the custom flow: Whisper (with splitting) -> OpenAI (model unspecified) -> Gemini 1.5 Flash.  A new `processAudioV2` function has been implemented to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  A new `generateSummaryWithKnowledgeBase` function has been implemented to generate summaries using a knowledge base.  The knowledge base is now populated by an admin user via the `uploadKnowledge` function.  The `uploadKnowledge` function now includes an admin key for authentication. A new `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, and `getInstrumentTemplates` functions have been implemented for admin users to manage knowledge base entries.  The `getSummaryPromptByInstrument` function in `dify.ts` has been updated to use if statements instead of a switch statement to improve the accuracy of instrument matching.  The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions in `genkit.ts` have also been updated to use if statements for improved instrument matching. The `dify.ts` file and related functions have been removed from the project. The system now uses only the `genkit.ts` module for summary and tag generation. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary and tag generation. A new `generateSummaryWithKnowledge` function has been implemented to generate summaries using a knowledge base. The `generateSummaryWithKnowledgeBase` function now uses a knowledge base to generate summaries and tags.  A new `processAudioFileV2` function has been implemented to handle the audio file processing. A new `generateSummaryFromTranscriptionV2` function has been implemented to handle summary and tag generation.  The `processAudioV2` function now uses the `processAudioCustomFlow` function for the custom flow. The `processAudioCustomFlow` function now uses a knowledge base for summary generation. A new `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, and `getInstrumentTemplates` functions have been implemented for admin users to manage knowledge base entries.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three comma-separated words.  The current workflow is: Audio Upload -> `processAudioFileV2` (Whisper transcription with splitting) -> `generateSummaryFromTranscriptionV2` (Summary and Tag generation using Gemini). The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into 10-minute chunks with overlap for transcription to improve context and accuracy.  The summarization API has been changed to OpenAI's o3-mini model.  Long audio files are split into approximately 10-minute chunks with overlap for improved context and accuracy during transcription. The summarization API is now OpenAI's o3-mini model.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  A new admin section has been added to handle knowledge base management.  Admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) have been implemented to manage the knowledge base. The `generateSummaryFromTranscriptionV2` function now uses the `generateSummaryWithKnowledgeBase` function for summary generation. Long audio files are split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model.  Tags are now generated as three individual words.  Long audio files are now split into approximately 10-minute chunks with a 20-second overlap for transcription.  Whisper API is used for transcription. OpenAI's o3-mini model is used for summarization. Tags are generated as three individual words. If fewer than three words are generated, the instrument name will be used to fill any gaps.  The custom flow now uses OpenAI's o3-mini model for summarization.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. Long audio files are now split into approximately 10-minute chunks with a 20-second overlap for transcription using the Whisper API.  The summarization is now performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for improved context and accuracy during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  The `getSummaryPromptByInstrument` function now uses if statements for improved instrument matching. The `getSummaryPromptForGemini` and `getTagsPromptForGemini` functions now use if statements for improved instrument matching. The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` if the knowledge base is enabled. Tags are now generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Long audio files are now split into approximately 10-minute chunks with a 20-second overlap for transcription. The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `processAudioCustomFlow` to handle the custom flow: Whisper (with splitting) -> OpenAI (o3-mini model) -> Gemini 1.5 Flash.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model. Long audio files are now split into approximately 10-minute chunks with a 20-second overlap for transcription using the Whisper API.  The summarization is now performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps. Long audio files are now split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  The custom flow now uses OpenAI's o3-mini model for summarization. A new admin section has been added to handle knowledge base management. Added a new function `processAudioV2` to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  The `processAudioCustomFlow` function now uses a knowledge base for summary generation.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new admin section to handle knowledge base management.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  Added a new admin section to handle knowledge base management. Added a new function `processAudioCustomFlow` to handle the custom flow: Whisper (with splitting) -> OpenAI (o3-mini model) -> Gemini 1.5 Flash.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model.  A new admin section has been added to handle knowledge base management. Added a new function `processAudioV2` to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  The `processAudioCustomFlow` function now uses a knowledge base for summary generation.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new admin section to handle knowledge base management.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  Added a new admin section to handle knowledge base management. Added a new function `processAudioCustomFlow` to handle the custom flow: Whisper (with splitting) -> OpenAI (o3-mini model) -> Gemini 1.5 Flash.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model.  A new admin section has been added to handle knowledge base management. Added a new function `processAudioV2` to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  The `processAudioCustomFlow` function now uses a knowledge base for summary generation.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new admin section to handle knowledge base management.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  Added a new admin section to handle knowledge base management. Added a new function `processAudioCustomFlow` to handle the custom flow: Whisper (with splitting) -> OpenAI (o3-mini model) -> Gemini 1.5 Flash.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model.  A new admin section has been added to handle knowledge base management. Added a new function `processAudioV2` to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  The `processAudioCustomFlow` function now uses a knowledge base for summary generation.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new admin section to handle knowledge base management.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  Added a new admin section to handle knowledge base management. Added a new function `processAudioCustomFlow` to handle the custom flow: Whisper (with splitting) -> OpenAI (o3-mini model) -> Gemini 1.5 Flash.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model.  A new admin section has been added to handle knowledge base management. Added a new function `processAudioV2` to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  The `processAudioCustomFlow` function now uses a knowledge base for summary generation.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new admin section to handle knowledge base management.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words. Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base.  Added a new admin section to handle knowledge base management. Added a new function `processAudioCustomFlow` to handle the custom flow: Whisper (with splitting) -> OpenAI (o3-mini model) -> Gemini 1.5 Flash.  The audio is now split into approximately 10-minute chunks with a 20-second overlap to maintain context during transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into 10-minute chunks with overlap for transcription. The summarization API has been changed to OpenAI's o3-mini model.  A new admin section has been added to handle knowledge base management. Added a new function `processAudioV2` to dynamically select the appropriate processing flow based on user settings.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  The `processAudioCustomFlow` function now uses a knowledge base for summary generation.  The `processAudioV2` function now calls `processAudioCustomFlow` or `processAudioDynamic` depending on the user's selected flow type.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new function `generateSummaryWithKnowledgeBase` for generating summaries using a knowledge base. Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added new admin functions: `adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates` for managing the knowledge base.  Added a new admin section to handle knowledge base management.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  Added new admin functions (`adminListKnowledge`, `adminGetKnowledge`, `adminDeleteKnowledge`, `getInstrumentTemplates`) to manage the knowledge base.  The `generateSummaryFromTranscriptionV2` function now uses `generateSummaryWithKnowledgeBase` for summary generation.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription using Whisper.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o3-mini model. Tags are generated as three individual words.  The transcription is performed using the OpenAI Whisper API. The summarization is performed using the OpenAI o3-mini model. Tags are generated as three individual words. If fewer than three words are generated, the instrument name is used to fill any gaps.  For audio files larger than 30MB, a warning will be displayed if not on Wi-Fi.  A warning will be displayed to the user if they are attempting to upload a file larger than 30MB over a non-Wi-Fi network. Long audio files are split into approximately 10-minute chunks with a 20-second overlap for transcription.  The summarization API is now OpenAI's o